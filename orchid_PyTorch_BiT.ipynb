{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXhZm0kpPpH6"
   },
   "source": [
    "##### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KfmzfvFxPuk7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlyBC_2SNzEH"
   },
   "source": [
    "# BigTransfer (BiT): A step-by-step tutorial for state-of-the-art vision\n",
    "\n",
    "This colab demonstrates how to:\n",
    "1. Load BiT models in PyTorch\n",
    "2. Make predictions using BiT pre-trained on ImageNet\n",
    "3. Fine-tune BiT on 5-shot CIFAR10 and get amazing results!\n",
    "\n",
    "It is good to get an understanding or quickly try things. However, to run longer training runs, we recommend using the commandline scripts at http://github.com/google-research/big_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MCUeLqhqh6d",
    "outputId": "f49ee592-e2ba-48b7-afe8-d9bc8492c809"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg2boM5BfvpF"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKYlZVpGo9jZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import datasets,transforms\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gW-XMhiVi0nh",
    "outputId": "a4c5d4b5-378f-478d-d47d-a5943c4c81a6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOVCm4CnP1Do"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLXOMGkV2_PW",
    "outputId": "d6666f64-cf65-44bc-9298-fdc2c220b1c3"
   },
   "source": [
    "# Only execution through Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Colab Notebooks/ai cup/\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk_SLaLsxv-r"
   },
   "source": [
    "# Modify train and val dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7hgDzJ3xrm9"
   },
   "outputs": [],
   "source": [
    "path = \"./dataset/training/\"\n",
    "for x in [\"train\",'val']:\n",
    "    for y in range(219):\n",
    "        l = os.listdir(path+x+\"/\"+str(y))\n",
    "        for img in l:\n",
    "            os.rename(path+x+\"/\"+str(y)+\"/\"+img,path+\"train/\"+str(y)+\"/\"+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPBDyLHbyIW7"
   },
   "outputs": [],
   "source": [
    "val_qty = 2 # In Validation dataset only have val_qty images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtAszACiyVGy"
   },
   "outputs": [],
   "source": [
    "path = \"./dataset/training/train/\"\n",
    "for y in range(219):\n",
    "    l = os.listdir(path+str(y))\n",
    "    for img in random.sample(l,k=val_qty):\n",
    "        os.rename(path+str(y)+\"/\"+img,\"./dataset/training/val/\"+str(y)+\"/\"+img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qyUoiTeUVQR"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZhYJ5oo2va0",
    "outputId": "612d8012-d8fc-47d3-ff0c-083dc4422bcc"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        tv.transforms.Resize((224, 224), interpolation=PIL.Image.BILINEAR),  # It's the default, just being explicit for the reader.\n",
    "        tv.transforms.CenterCrop((212, 212)), # Method (8) Don't centre crop\n",
    "        tv.transforms.RandomHorizontalFlip(),\n",
    "        #tv.transforms.RandomCrop(size = (200,200)), # Method (6)\n",
    "        tv.transforms.RandomResizedCrop((200,200)), # Method (7) & (8)\n",
    "        #tv.transforms.RandomAutocontrast(p=0.9),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Get data into [-1, 1]\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        tv.transforms.Resize((224, 224), interpolation=PIL.Image.BILINEAR),\n",
    "        tv.transforms.CenterCrop((200, 200)),\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-8A26U_mLBQ"
   },
   "outputs": [],
   "source": [
    "def map_ans(target,class_names):\n",
    "    if target <219:\n",
    "        return int(class_names[target])\n",
    "    else:\n",
    "        return 219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RudLA30fP86c"
   },
   "outputs": [],
   "source": [
    "def map_key(target,class_names):\n",
    "    for i,cls in enumerate(class_names):\n",
    "        if str(target) == cls:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDVEzjDD2HhU"
   },
   "outputs": [],
   "source": [
    "data_dir = './dataset/training/'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                  data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CU2aAi-4QZEG",
    "outputId": "92385910-776a-4df8-cc94-e8c0b2e36478"
   },
   "outputs": [],
   "source": [
    "## Remove image wish to be igonored\n",
    "dataset_sizes_before_removal= {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "ignore_list = [] #If want to ignore imagehash method\n",
    "for ign in ignore_list:\n",
    "    ign_cls = ign[:ign.rfind('/')]\n",
    "    ign_cls = ign_cls[ign_cls.rfind('/')+1:]\n",
    "    ign_tuple=(ign,map_key(ign_cls,class_names))\n",
    "    image_datasets['train'].imgs.remove(ign_tuple)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Train dataset size before remove:%d\"%dataset_sizes_before_removal['train'])\n",
    "print(\"Remove total:%d\"%len(ignore_list))\n",
    "print(\"Train dataset size after remove:%d\"%dataset_sizes['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbKA7-xhUQMr"
   },
   "source": [
    "# Find indices to create a 5-shot ORCHID variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRPkGcvTUWAR",
    "outputId": "e5131bbb-54ae-44a0-f401-8cb0d09d85f6"
   },
   "outputs": [],
   "source": [
    "#preprocess_tiny = tv.transforms.Compose([tv.transforms.CenterCrop((2, 2)), tv.transforms.ToTensor()])\n",
    "#trainset_tiny = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=preprocess_tiny)\n",
    "val_qty = 2\n",
    "shots = 10-val_qty#5\n",
    "showqty = 10\n",
    "loader_train = torch.utils.data.DataLoader(image_datasets['train'], batch_size=100, shuffle=False, num_workers=2) #batch size > showqty*shots\n",
    "images, labels = iter(loader_train).next()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93kwltfSN5sk",
    "outputId": "3e1e4288-0297-4c19-ffe7-9299769aaccb"
   },
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "bz1i4UQoUhWW",
    "outputId": "b772fe4d-837f-485a-953f-3248d42e3567"
   },
   "outputs": [],
   "source": [
    "indices = {cls: np.random.choice(np.where(labels.numpy() == cls)[0], shots, replace=False) for cls in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vnq22Qy0jmBR",
    "outputId": "279ddac7-334d-4ba6-93da-758e8bc4db4a"
   },
   "outputs": [],
   "source": [
    "print(indices)\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g9Gn5a_NWV4k",
    "outputId": "d79b60b8-ce32-4afa-df39-fe8f7352852e"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40, 16))\n",
    "ig = ImageGrid(fig, 111, (showqty, shots))\n",
    "for c, cls in enumerate(indices):\n",
    "    if c>=showqty:\n",
    "        break\n",
    "    for r, i in enumerate(indices[cls]):\n",
    "        if r>=shots:\n",
    "            continue\n",
    "        img, _ = image_datasets['train'][i]\n",
    "        ax = ig.axes_column[r][c]\n",
    "        ax.imshow((img.numpy().transpose([1, 2, 0]) * 127.5 + 127.5).astype(np.uint8))\n",
    "        ax.set_axis_off()\n",
    "fig.suptitle('%d of the %d-shot ORCHID dataset'%(showqty,shots));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vvu5XXCQY88M",
    "outputId": "714f4271-3740-4817-e823-d1c69e5ba25d"
   },
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=2000, shuffle=False, num_workers=2)\n",
    "images, labels = iter(loader).next()\n",
    "indices = {cls: np.random.choice(np.where(labels.numpy() == cls)[0], shots, replace=False) for cls in range(219)}\n",
    "train_5shot = torch.utils.data.Subset(image_datasets['train'], indices=[i for v in indices.values() for i in v])\n",
    "len(train_5shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_lOBZYwy1SW"
   },
   "source": [
    "# Reading weight data from the Cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg5HNzLyzubz"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC6YhfLb1ZGE"
   },
   "outputs": [],
   "source": [
    "def get_weights(bit_variant):\n",
    "    response = requests.get(f'https://storage.googleapis.com/bit_models/{bit_variant}.npz')\n",
    "    response.raise_for_status()\n",
    "    return np.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exuy13Igwa_X"
   },
   "outputs": [],
   "source": [
    "weights = get_weights('vtab/BiT-M-R101x1-run1-oxford_flowers102')  # You could use other variants, such as R101x3 or R152x4 here, but it is not advisable in a colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWzT0g8x6feo"
   },
   "source": [
    "# Defining the architecture and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y00wwFQvrwsX"
   },
   "outputs": [],
   "source": [
    "class StdConv2d(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-10)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgXeQXKVpPqV"
   },
   "outputs": [],
   "source": [
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride, padding=1, bias=bias, groups=groups)\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride, padding=0, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4yzODhR6U6p"
   },
   "outputs": [],
   "source": [
    "def tf2th(conv_weights):\n",
    "    \"\"\"Possibly convert HWIO to OIHW\"\"\"\n",
    "    if conv_weights.ndim == 4:\n",
    "        conv_weights = np.transpose(conv_weights, [3, 2, 0, 1])\n",
    "    return torch.from_numpy(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNq34tmsqgXG"
   },
   "outputs": [],
   "source": [
    "class PreActBottleneck(nn.Module):\n",
    "  \"\"\"\n",
    "  Follows the implementation of \"Identity Mappings in Deep Residual Networks\" here:\n",
    "  https://github.com/KaimingHe/resnet-1k-layers/blob/master/resnet-pre-act.lua\n",
    "\n",
    "  Except it puts the stride on 3x3 conv when available.\n",
    "  \"\"\"\n",
    "  def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "    super().__init__()\n",
    "    cout = cout or cin\n",
    "    cmid = cmid or cout//4\n",
    "\n",
    "    self.gn1 = nn.GroupNorm(32, cin)\n",
    "    self.conv1 = conv1x1(cin, cmid)\n",
    "    self.gn2 = nn.GroupNorm(32, cmid)\n",
    "    self.conv2 = conv3x3(cmid, cmid, stride)  # Original ResNetv2 has it on conv1!!\n",
    "    self.gn3 = nn.GroupNorm(32, cmid)\n",
    "    self.conv3 = conv1x1(cmid, cout)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    if (stride != 1 or cin != cout):\n",
    "      # Projection also with pre-activation according to paper.\n",
    "      self.downsample = conv1x1(cin, cout, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv'ed branch\n",
    "        out = self.relu(self.gn1(x))\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(out)\n",
    "\n",
    "        # The first block has already applied pre-act before splitting, see Appendix.\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.relu(self.gn2(out)))\n",
    "        out = self.conv3(self.relu(self.gn3(out)))\n",
    "\n",
    "        return out + residual\n",
    "\n",
    "    def load_from(self, weights, prefix=''):\n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight.copy_(tf2th(weights[prefix + 'a/standardized_conv2d/kernel']))\n",
    "            self.conv2.weight.copy_(tf2th(weights[prefix + 'b/standardized_conv2d/kernel']))\n",
    "            self.conv3.weight.copy_(tf2th(weights[prefix + 'c/standardized_conv2d/kernel']))\n",
    "            self.gn1.weight.copy_(tf2th(weights[prefix + 'a/group_norm/gamma']))\n",
    "            self.gn2.weight.copy_(tf2th(weights[prefix + 'b/group_norm/gamma']))\n",
    "            self.gn3.weight.copy_(tf2th(weights[prefix + 'c/group_norm/gamma']))\n",
    "            self.gn1.bias.copy_(tf2th(weights[prefix + 'a/group_norm/beta']))\n",
    "            self.gn2.bias.copy_(tf2th(weights[prefix + 'b/group_norm/beta']))\n",
    "            self.gn3.bias.copy_(tf2th(weights[prefix + 'c/group_norm/beta']))\n",
    "            if hasattr(self, 'downsample'):\n",
    "                self.downsample.weight.copy_(tf2th(weights[prefix + 'a/proj/standardized_conv2d/kernel']))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fGKjfXm60pY"
   },
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Module):\n",
    "    BLOCK_UNITS = {\n",
    "      'r50': [3, 4, 6, 3],\n",
    "      'r101': [3, 4, 23, 3],\n",
    "      'r152': [3, 8, 36, 3],\n",
    "    }\n",
    "\n",
    "    def __init__(self, block_units, width_factor, head_size=21843, zero_head=False):\n",
    "        super().__init__()\n",
    "        wf = width_factor  # shortcut 'cause we'll use it a lot.\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, 64*wf, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('padp', nn.ConstantPad2d(1, 0)),\n",
    "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n",
    "            # The following is subtly not the same!\n",
    "            #('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 64*wf, cout=256*wf, cmid=64*wf))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=256*wf, cout=256*wf, cmid=64*wf)) for i in range(2, block_units[0] + 1)],\n",
    "            ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=256*wf, cout=512*wf, cmid=128*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=512*wf, cout=512*wf, cmid=128*wf)) for i in range(2, block_units[1] + 1)],\n",
    "            ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 512*wf, cout=1024*wf, cmid=256*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=1024*wf, cout=1024*wf, cmid=256*wf)) for i in range(2, block_units[2] + 1)],\n",
    "            ))),\n",
    "            ('block4', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=1024*wf, cout=2048*wf, cmid=512*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=2048*wf, cout=2048*wf, cmid=512*wf)) for i in range(2, block_units[3] + 1)],\n",
    "            ))),\n",
    "        ]))\n",
    "\n",
    "        self.zero_head = zero_head\n",
    "        self.head = nn.Sequential(OrderedDict([\n",
    "            ('gn', nn.GroupNorm(32, 2048*wf)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            ('avg', nn.AdaptiveAvgPool2d(output_size=1)),\n",
    "            ('conv', nn.Conv2d(2048*wf, head_size, kernel_size=1, bias=True)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(self.body(self.root(x)))\n",
    "        assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n",
    "        return x[...,0,0]\n",
    "\n",
    "    def load_from(self, weights, prefix='resnet/'):\n",
    "        with torch.no_grad():\n",
    "            self.root.conv.weight.copy_(tf2th(weights[f'{prefix}root_block/standardized_conv2d/kernel']))\n",
    "            self.head.gn.weight.copy_(tf2th(weights[f'{prefix}group_norm/gamma']))\n",
    "            self.head.gn.bias.copy_(tf2th(weights[f'{prefix}group_norm/beta']))\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.conv.weight)\n",
    "                nn.init.zeros_(self.head.conv.bias)\n",
    "            else:\n",
    "                self.head.conv.weight.copy_(tf2th(weights[f'{prefix}head/conv2d/kernel']))\n",
    "                self.head.conv.bias.copy_(tf2th(weights[f'{prefix}head/conv2d/bias']))\n",
    "\n",
    "            for bname, block in self.body.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, prefix=f'{prefix}{bname}/{uname}/')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDMzHsUFsmRu"
   },
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk7RAE3bsntF"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wSnf5r8V4-T"
   },
   "outputs": [],
   "source": [
    "def stairs(s, v, *svs):\n",
    "    \"\"\" Implements a typical \"stairs\" schedule for learning-rates.\n",
    "    Best explained by example:\n",
    "    stairs(s, 0.1, 10, 0.01, 20, 0.001)\n",
    "    will return 0.1 if s<10, 0.01 if 10<=s<20, and 0.001 if 20<=s\n",
    "    \"\"\"\n",
    "    for s0, v0 in zip(svs[::2], svs[1::2]):\n",
    "        if s < s0:\n",
    "            break\n",
    "        v = v0\n",
    "    return v\n",
    "\n",
    "def rampup(s, peak_s, peak_lr):\n",
    "    if s < peak_s:  # Warmup\n",
    "        return s/peak_s * peak_lr\n",
    "    else:\n",
    "        return peak_lr\n",
    "\n",
    "def schedule(s):\n",
    "    step_lr = stairs(s, 3e-3, 200, 3e-4, 300, 3e-5, 400, 3e-6, 500, None)\n",
    "    return rampup(s, 100, step_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7aYBd532f0n"
   },
   "source": [
    "# Eval pre-trained model (verify conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFpmWKNf2ijx"
   },
   "outputs": [],
   "source": [
    "def eval(model, bs=100, progressbar=True):\n",
    "    loader_test = torch.utils.data.DataLoader(image_datasets['val'], batch_size=bs, shuffle=False, num_workers=2)\n",
    "    model.eval()\n",
    "    if progressbar is True:\n",
    "        progressbar = display(progress(0, len(loader_test)), display_id=True)\n",
    "\n",
    "    accus = []\n",
    "    preds = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, t) in enumerate(loader_test):\n",
    "            x, t = x.to(device), t.numpy()\n",
    "            logits = model(x)\n",
    "            prob, y = torch.max(logits, 1)\n",
    "            preds.extend(y.cpu().numpy())\n",
    "            true.extend(t)\n",
    "            accus.extend(y.cpu().numpy() == t)\n",
    "            progressbar.update(progress(i+1, len(loader_test)))\n",
    "    m_accus = np.mean(accus)\n",
    "\n",
    "    m_f1 = f1_score(true,preds,average = 'macro')\n",
    "    return m_accus,m_f1,0.5*m_accus + 0.5*m_f1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMrZ1sZyUVcU"
   },
   "source": [
    "# Fine-tune BiT-M on this 5-shot CIFAR10 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qCZ4SiBfV32"
   },
   "source": [
    "**NOTE**: In this very low data regime, the performance heavily depends on how \"representative\" the 5 examples you got are of the class. As shown in the paper, variance is very large, I'm getting anywhere between 78%-85% depending on luck.\n",
    "\n",
    "Another point is that here I use `batch_size=512` for consistency with the paper. But actually, a much smaller `batch_size=50` works just as well and is about 10x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfyVoCMxVcwn",
    "outputId": "dee22126-e6e2-40a4-aabe-a9918254828a"
   },
   "outputs": [],
   "source": [
    "model = ResNetV2(ResNetV2.BLOCK_UNITS['r101'], width_factor=1, head_size=219, zero_head=True)\n",
    "#weights = get_weights('vtab/BiT-M-R101x1-run1-oxford_flowers102')\n",
    "#model.load_from(weights)\n",
    "model.load_state_dict(torch.load('best_7.pth'))\n",
    "model.to(device);\n",
    "summary(model,(3, 80, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "faASYtOkYMrn",
    "outputId": "a7af88d3-0ded-4d84-969f-bd8cb190c457"
   },
   "outputs": [],
   "source": [
    "# Yes, we still use 512 batch-size! Maybe something else is even better, who knows.\n",
    "# loader_train = torch.utils.data.DataLoader(train_5shot, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "# NOTE: This is necessary when the batch-size is larger than the dataset.\n",
    "sampler = torch.utils.data.RandomSampler(train_5shot, replacement=True, num_samples=256)\n",
    "loader_train = torch.utils.data.DataLoader(train_5shot, batch_size=10, num_workers=2, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqLUFicBVcu2"
   },
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "opti = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9,weight_decay= 1e-4)\n",
    "#opti = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay= 1e-4)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHmMNYpM6aMx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "RVmFu5m1eJa7",
    "outputId": "3603a3ab-c884-4067-ffb8-6d885b07a677"
   },
   "outputs": [],
   "source": [
    "S = 300\n",
    "def schedule(s):\n",
    "    step_lr = stairs(s, 3e-3, 200, 3e-4, 300, 3e-5, 400, 3e-6, S, None)\n",
    "    return rampup(s, 100, step_lr)\n",
    "\n",
    "pb_train = display(progress(0, S), display_id=True)\n",
    "pb_test = display(progress(0, 100), display_id=True)\n",
    "losses = [[]]\n",
    "accus_train = [[]]\n",
    "accus_test = []\n",
    "final_score_train = []\n",
    "final_score_test = []\n",
    "best_fs = 0\n",
    "\n",
    "steps_per_iter = 512 // loader_train.batch_size\n",
    "\n",
    "while len(losses) < S:\n",
    "    trues = []\n",
    "    preds = []\n",
    "    for x, t in loader_train:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = crit(logits, t) / steps_per_iter\n",
    "    loss.backward()\n",
    "    losses[-1].append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true = t.cpu().numpy()\n",
    "        pred = torch.max(logits, dim=1)[1].cpu().numpy()\n",
    "        trues.extend(true)\n",
    "        preds.extend(pred)\n",
    "        accus_train[-1].extend(pred == true)\n",
    "    if len(losses[-1]) == steps_per_iter:\n",
    "        losses[-1] = sum(losses[-1])\n",
    "        losses.append([])\n",
    "        accus_train[-1] = np.mean(accus_train[-1])\n",
    "        accus_train.append([])\n",
    "        #f1 = f1_score(trues,preds,average = 'macro')\n",
    "        try:\n",
    "            f1 = f1_score(trues,preds,average = 'macro')\n",
    "        except:\n",
    "            print(\"F1 score calculation error!\")\n",
    "            print(trues)\n",
    "            print(preds)\n",
    "            print(len(trues))\n",
    "            print(len(preds))\n",
    "            f1 = 0\n",
    "        final_score_train.append(0.5*accus_train[-2]+0.5*f1)\n",
    "        # Update learning-rate according to schedule, and stop if necessary\n",
    "        lr = schedule(len(losses) - 1)\n",
    "        for param_group in opti.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        opti.step()\n",
    "        opti.zero_grad()\n",
    "\n",
    "        pb_train.update(progress(len(losses) - 1, S))\n",
    "        print(f'\\r[Step {len(losses) - 1}] loss={losses[-2]:.2e} '\n",
    "              f'train final score={final_score_train[-1]:.2%} '\n",
    "              f'test final score={final_score_test[-1] if final_score_test else 0:.2%} '\n",
    "              f'train accu={accus_train[-2]:.2%} '\n",
    "              f'test accu={accus_test[-1] if accus_test else 0:.2%} '\n",
    "              f'(lr={lr:g})', end='', flush=True)\n",
    "        if len(losses) % 25 == 0:\n",
    "            a,_,fs = eval(model, progressbar=pb_test)\n",
    "            final_score_test.append(fs)\n",
    "            accus_test.append(a)\n",
    "            if fs>best_fs:\n",
    "                torch.save(model.state_dict(),\"best_\"+\"5.pth\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UA81fDGGRpNo"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "TZQBHq1Gh1ty",
    "outputId": "271b9e53-4c40-499c-ae27-48e365891fa8"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "ax1.plot(losses[:-1])\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax2.plot(final_score_train[:-1])\n",
    "ax2.set_title('training final score')\n",
    "ax3.plot(np.arange(25, S+1, 25), final_score_test)\n",
    "ax3.set_title('test final score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tBUgmUpADye"
   },
   "outputs": [],
   "source": [
    "def eval_one(model,x,t):\n",
    "    model.eval()\n",
    "    cmap = cm.jet(np.linspace(0, 1, 3))\n",
    "    with torch.no_grad():\n",
    "        x, t = x.to(device), t.numpy()\n",
    "        logits = model(x)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        result, y = torch.max(probs, 1)     \n",
    "        fig,ax= plt.subplots(ncols=2, nrows=2,figsize = (10,10))\n",
    "        Y = map_ans(y.cpu().numpy()[0],class_names)\n",
    "        T = map_ans(t[0],class_names)\n",
    "        ax[0,0].set_title(\"Predict:%d Correct:%d\"%(Y,T))\n",
    "        ax[0,0].imshow((x[0].cpu().numpy().transpose([1, 2, 0]) * 127.5 + 127.5).astype(np.uint8))\n",
    "\n",
    "        ax[0,1].set_title(\"Correct class sample:%d\"%T)\n",
    "        t_ind = indices[t[0]]\n",
    "        ax[0,1].imshow((image_datasets['train'][t_ind[0]][0].numpy().transpose([1, 2, 0]) * 127.5 + 127.5).astype(np.uint8))\n",
    "\n",
    "        ax[1,0].set_title(\"Predict class sample:%d\"%Y)\n",
    "        p_ind = indices[y.cpu().numpy()[0]]\n",
    "        ax[1,0].imshow((image_datasets['train'][p_ind[0]][0].numpy().transpose([1, 2, 0]) * 127.5 + 127.5).astype(np.uint8))\n",
    "\n",
    "        prob,cls = torch.topk(probs, 3, dim=1)\n",
    "        a = np.arange(3)\n",
    "        ax[1,1].barh(a, prob[0].cpu().numpy(), color=cmap)\n",
    "        '''\n",
    "        if prob[0].cpu().numpy()[0]<0.5:\n",
    "          ax[1,1].set_title(\"Unrecognized flower!\")\n",
    "        '''\n",
    "        cls = cls.tolist()[0]\n",
    "        for i in range(len(cls)):\n",
    "            cls[i] = map_ans(cls[i],class_names)\n",
    "        ax[1,1].set_yticks(a)\n",
    "        ax[1,1].set_yticklabels(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkUtoAsoO4kE",
    "outputId": "99f33b39-1e19-4177-e153-8ecedf4a8349"
   },
   "outputs": [],
   "source": [
    "model = ResNetV2(ResNetV2.BLOCK_UNITS['r101'], width_factor=1, head_size=219, zero_head=True)\n",
    "model.load_state_dict(torch.load('./pth/best_8.pth'))\n",
    "model.to(device);\n",
    "pb_test = display(progress(0, 100), display_id=True)\n",
    "eval(model,progressbar = pb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQU8vrDNX26g"
   },
   "source": [
    "# Result\n",
    "Method 5:\n",
    "(0.8458049886621315, 0.8303326810176125, 0.8380688348398719)\n",
    "\n",
    "Method 6:\n",
    "(0.8684807256235828, 0.8589693411611219, 0.8637250333923523)\n",
    "\n",
    "Method 7:\n",
    "(0.8934240362811792, 0.8811480756686237, 0.8872860559749014)\n",
    "\n",
    "Method 8: \n",
    "(0.8888888888888888, 0.8772124374864101, 0.8830506631876495)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sSTiR_gOfiF"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjDENTm3Or6D"
   },
   "outputs": [],
   "source": [
    "image_dataset = dict()\n",
    "image_datasets['test'] = datasets.ImageFolder('./dataset/training/test/',data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEjkxnrgOd5w"
   },
   "outputs": [],
   "source": [
    "with open(\"submission_8.csv\",'w', encoding='UTF8', newline='') as f:\n",
    "    model.eval()\n",
    "    loader_test = torch.utils.data.DataLoader(image_datasets['test'], batch_size=1, shuffle=False, num_workers=2)\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['filename','category'])\n",
    "    preds = []\n",
    "    # write the data\n",
    "    for i,(x,t) in enumerate(loader_test):\n",
    "        print('\\r %d'%i,end=\"\")\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        prob,y = torch.max(probs, 1)\n",
    "        preds.extend(y.cpu().numpy())\n",
    "        '''\n",
    "        if prob[0]<0.5:\n",
    "          preds.extend(np.array([219]))\n",
    "        else:\n",
    "          preds.extend(y.cpu().numpy())\n",
    "        '''\n",
    "    for i,(x,y) in enumerate(zip(image_datasets['test'].imgs,preds)):\n",
    "        filename = x[0]\n",
    "        filename = filename[filename.rfind(\"/\")+1:]\n",
    "        writer.writerow([filename,map_ans(y,class_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3icoBdDO6kO",
    "outputId": "bbfa4e8c-3f18-4f33-dd31-8b889c230ad4"
   },
   "outputs": [],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRqfuuoXPhD8",
    "outputId": "15396f71-487f-4f8d-f822-a0caa2218bcd"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir(\"./dataset/training/test/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irh1OptpRL5z"
   },
   "source": [
    "# Validation Result View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qhy2ZiFyERKr"
   },
   "outputs": [],
   "source": [
    "with open(\"submission_7.csv\",'w', encoding='UTF8', newline='') as f:\n",
    "    model.eval()\n",
    "    loader_test = torch.utils.data.DataLoader(image_datasets['val'], batch_size=50, shuffle=False, num_workers=2)\n",
    "    writer = csv.writer(f)\n",
    "    preds = []\n",
    "    true = []\n",
    "    wrong_pred = []\n",
    "    # write the data\n",
    "    for i,(x,t) in enumerate(loader_test):\n",
    "        x, t = x.to(device), t.numpy()\n",
    "        logits = model(x)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        prob,y = torch.max(probs, 1)\n",
    "        '''\n",
    "        if prob[0]<0.5:\n",
    "            preds.extend(np.array([219]))\n",
    "        else:\n",
    "            preds.extend(y.cpu().numpy())\n",
    "        '''\n",
    "        preds.extend(y.cpu().numpy())\n",
    "        true.extend(t)\n",
    "    for i,(x,y,z) in enumerate(zip(image_datasets['val'].imgs,preds,true)):\n",
    "        filename = x[0]\n",
    "        filename = filename[filename.rfind(\"/\")+1:filename.rfind(\".\")]\n",
    "        writer.writerow([filename,map_ans(y,class_names),map_ans(z,class_names)])\n",
    "        if map_ans(y,class_names)!=map_ans(z,class_names):\n",
    "            wrong_pred.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVeIJUZ37jK7",
    "outputId": "9cf81442-31bd-4a2b-c4b5-e90ab56665f7"
   },
   "outputs": [],
   "source": [
    "print(wrong_pred)\n",
    "print(len(wrong_pred))\n",
    "print(219*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "cXfvT6M7PBOh",
    "outputId": "0f5c0cba-402e-4632-f334-8fe8205ee71a"
   },
   "outputs": [],
   "source": [
    "loader_test = torch.utils.data.DataLoader(image_datasets[\"val\"], batch_size=1, shuffle=False, num_workers=2)\n",
    "for i, (x, t) in enumerate(loader_test):\n",
    "    if i not in wrong_pred:\n",
    "        continue\n",
    "    eval_one(model,x,t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HlyBC_2SNzEH",
    "Uk_SLaLsxv-r",
    "VMjWcG1_ysiV",
    "7qyUoiTeUVQR",
    "C_lOBZYwy1SW",
    "mWzT0g8x6feo",
    "zDMzHsUFsmRu",
    "V7aYBd532f0n"
   ],
   "machine_shape": "hm",
   "name": "orchid PyTorch BiT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
